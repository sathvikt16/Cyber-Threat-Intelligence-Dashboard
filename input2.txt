orchestrator.py : 

import schedule
import time
from agents import DataIngestionAgent, IntelligenceExtractionAgent, IoCAnalysisAgent, PersistenceAgent, DiscoveryAgent # <-- Add DiscoveryAgent
from database.db_handler import init_db
from processor.vertex_ai_processor import check_ai_health

def main_workflow():
    """
    The main orchestration function that coordinates all agents.
    It runs a two-stage process for unstructured and structured data.
    """
    print("\n" + "="*50 + f"\nORCHESTRATOR: Starting new workflow run at {time.ctime()}\n" + "="*50)
    
    # Initialize all necessary agents for the workflow
    ingestion_agent = DataIngestionAgent()
    extraction_agent = IntelligenceExtractionAgent()
    analysis_agent = IoCAnalysisAgent()
    persistence_agent = PersistenceAgent()

    # --- Step 1: Process Unstructured Data (Needs AI) ---
    print("\n--- Processing Unstructured Sources (CISA, NIST, News) ---")
    raw_cisa = ingestion_agent.ingest_cisa()
    
    # Increased limit for The Hacker News to get more data
    raw_thn = ingestion_agent.ingest_hacker_news(limit=10)
    
    raw_nist = ingestion_agent.ingest_nist_nvd(days=2)
    unstructured_raw_data = raw_cisa + raw_thn + raw_nist
    
    print(f"\nORCHESTRATOR: Ingested {len(unstructured_raw_data)} unstructured items for AI processing.")

    for raw_item in unstructured_raw_data:
        # Pass through the AI processing pipeline
        pulse = extraction_agent.process_raw_data(raw_item)
        if pulse:
            # Enrich indicators if any are found
            pulse['indicators'] = analysis_agent.enrich_indicators(pulse.get('indicators', []))
            # Save the final, processed pulse to the database
            persistence_agent.save_pulse(pulse)
            print("-" * 20)

    # --- Step 2: Process Structured Data from OTX (Bypasses AI) ---
    print("\n--- Processing Structured Source (AlienVault OTX) ---")
    otx_pulses = ingestion_agent.ingest_otx(limit=15)
    print(f"\nORCHESTRATOR: Ingested {len(otx_pulses)} pre-structured OTX pulses.")
    
    for pulse in otx_pulses:
        # Data is already structured, so we just enrich and save
        pulse['indicators'] = analysis_agent.enrich_indicators(pulse.get('indicators', []))
        persistence_agent.save_pulse(pulse)
        print("-" * 20)

    print("\nORCHESTRATOR: Workflow run finished.")

# --- NEW: A separate job function for the discovery agent ---
def discovery_workflow():
    """A dedicated workflow for discovering new sources."""
    print("\n" + "="*50 + f"\nORCHESTRATOR: Starting Discovery Workflow at {time.ctime()}\n" + "="*50)
    discovery_agent = DiscoveryAgent()
    discovery_agent.run_discovery_cycle()
    print("\nORCHESTRATOR: Discovery Workflow finished.")

def main():
    print("Initializing CTI Orchestrator...")
    if not check_ai_health():
        print("Aborting startup due to AI health check failure.")
        return
    init_db()
    
    # Run the main data collection workflow once immediately
    main_workflow()
    
    # Schedule the main workflow to run every 2 hours
    schedule.every(2).hours.do(main_workflow)
    
    # --- NEW: Schedule the discovery workflow to run once a day ---
    schedule.every().day.at("03:00").do(discovery_workflow) # Run at 3 AM
    
    print("\nOrchestrator started. Data collection and discovery jobs are scheduled.")
    print("Running initial discovery pass now...")
    discovery_workflow() # Also run discovery once on startup
    
    while True:
        schedule.run_pending()
        time.sleep(60)

if __name__ == "__main__":
    main()

